{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKrFVyALNd5W"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UQRE7O25LODV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from os import getcwd, listdir, mkdir, chdir\n",
    "from os.path import join\n",
    "import requests\n",
    "if 'colab' in str(get_ipython()): from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYteHT4QNhTO"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gI8zRSCf_Fmq"
   },
   "outputs": [],
   "source": [
    "length_generated_text = 1000\n",
    "seed = 'ยง'\n",
    "model_filename = 'model___run__2021-03-25__17-42-59___Stateful_GRU_LN_encoding_dense.h5'\n",
    "tokenizer_filename = 'tokenizer___run__2021-03-25__17-42-59___Stateful_GRU_LN_encoding_dense.json'\n",
    "model_directory = 'models'\n",
    "model_url = 'https://github.com/fabio-a-oliveira/14-CFR-FAA/blob/main/models/model___run__2021-03-25__17-42-59___Stateful_GRU_LN_encoding_dense.h5?raw=true'\n",
    "tokenizer_url = 'https://github.com/fabio-a-oliveira/14-CFR-FAA/blob/main/models/tokenizer___run__2021-03-25__17-42-59___Stateful_GRU_LN_encoding_dense.json?raw=true'\n",
    "\n",
    "generated_text_filename = time.strftime(\"generated_text___\" + str(length_generated_text) + \"_chars__%Y-%m-%d__%H-%M-%S\")\n",
    "generated_text_directory = 'generated_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV7UYZ3-Np49"
   },
   "source": [
    "# Functions and custom classes definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sX_LkPU9_VUh"
   },
   "outputs": [],
   "source": [
    "class LN_GRU_Cell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", dropout=0, recurrent_dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.GRU_cell = keras.layers.GRUCell(units, dropout=dropout, recurrent_dropout=recurrent_dropout, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.GRU_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [new_states]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        custom_config = {'units':self.units,\n",
    "                         'dropout':self.dropout,\n",
    "                         'recurrent_dropout':self.recurrent_dropout,\n",
    "                         'activation':self.activation}\n",
    "        return {**base_config, **custom_config}\n",
    "    \n",
    "class LN_LSTM_Cell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", dropout=0, recurrent_dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.state_size = [units, units]\n",
    "        self.output_size = units\n",
    "        self.LSTM_cell = keras.layers.LSTMCell(units, dropout=dropout, recurrent_dropout=recurrent_dropout, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        memory_states, carry_states = states\n",
    "        outputs, new_states = self.LSTM_cell(inputs, [memory_states, carry_states])\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [new_states]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        custom_config = {'units':self.units,\n",
    "                         'dropout':self.dropout,\n",
    "                         'recurrent_dropout':self.recurrent_dropout,\n",
    "                         'activation':self.activation}\n",
    "        return {**base_config, **custom_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mIMMkRBK-hA5"
   },
   "outputs": [],
   "source": [
    "def convert_to_inference_model(original_model, custom_objects=None):\n",
    "    original_model_json = original_model.to_json()\n",
    "    inference_model_dict = json.loads(original_model_json)\n",
    "\n",
    "    layers = inference_model_dict['config']['layers']\n",
    "    for layer in layers:\n",
    "        if 'stateful' in layer['config']:\n",
    "            layer['config']['stateful'] = True\n",
    "\n",
    "        if 'batch_input_shape' in layer['config']:\n",
    "            layer['config']['batch_input_shape'][0] = 1\n",
    "            layer['config']['batch_input_shape'][1] = None\n",
    "\n",
    "    inference_model = keras.models.model_from_json(json.dumps(inference_model_dict), custom_objects = custom_objects)\n",
    "    inference_model.set_weights(original_model.get_weights())\n",
    "\n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeO6AS6JNsSF"
   },
   "source": [
    "# Download required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bWP8G8f2N7Tz"
   },
   "outputs": [],
   "source": [
    "if model_directory not in listdir():\n",
    "    mkdir(model_directory)\n",
    "\n",
    "if model_filename not in listdir(model_directory):\n",
    "    wd = getcwd()\n",
    "    chdir(model_directory)\n",
    "    r = requests.get(model_url)\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    chdir(wd)\n",
    "\n",
    "if tokenizer_filename not in listdir(model_directory):\n",
    "    wd = getcwd()\n",
    "    chdir(model_directory)\n",
    "    r = requests.get(tokenizer_url)\n",
    "    with open(tokenizer_filename, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    chdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGfhkhfVNzPC"
   },
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-HpS-sDTXy5",
    "outputId": "15ed4d58-98b6-4e26-ec0e-db0170b7db78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Stateful_GRU_LN_encoding_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "TD_Encoding (TimeDistributed (None, None, 116)         0         \n",
      "_________________________________________________________________\n",
      "Recurrent_0 (RNN)            (None, None, 580)         1215680   \n",
      "_________________________________________________________________\n",
      "Recurrent_1 (RNN)            (None, None, 580)         2023040   \n",
      "_________________________________________________________________\n",
      "Recurrent_2 (RNN)            (None, None, 580)         2023040   \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, None, 116)         67396     \n",
      "=================================================================\n",
      "Total params: 5,329,156\n",
      "Trainable params: 5,329,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(join(getcwd(), model_directory, model_filename),\n",
    "                                   custom_objects = {'LN_GRU_Cell':LN_GRU_Cell, 'LN_LSTM_Cell':LN_LSTM_Cell})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vhwb2C5PTXvh"
   },
   "outputs": [],
   "source": [
    "with open(join(getcwd(), model_directory, tokenizer_filename), 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3vohZ67N185"
   },
   "source": [
    "# Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZPbiiwO9-vR0",
    "outputId": "6230a4cb-cecc-4009-b76c-6835086d616d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress -- characters generated: 0/1000\n"
     ]
    }
   ],
   "source": [
    "#inference_model = convert_to_inference_model(model, custom_objects = {'LN_GRU_Cell':LN_GRU_Cell, 'LN_LSTM_Cell':LN_LSTM_Cell})\n",
    "inference_model = model\n",
    "\n",
    "\n",
    "text = seed\n",
    "sequence = tokenizer.texts_to_sequences([text])\n",
    "dict_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "for i in range(length_generated_text):\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print('Progress -- characters generated: {}/{}'.format(i, length_generated_text))\n",
    "    elif i == length_generated_text-1:\n",
    "        print('Progress -- characters generated: {}/{}'.format(length_generated_text, length_generated_text))\n",
    "\n",
    "    probs = inference_model.predict(np.array(sequence[0][-1]).reshape(1,-1,1))\n",
    "    token = np.random.choice(np.arange(dict_size), p = probs[0,-1,:])\n",
    "    sequence[0].append(token)\n",
    "\n",
    "text = tokenizer.sequences_to_texts(sequence)[0]    \n",
    "print(text[0:np.min([2000, length_generated_text]):2])\n",
    "\n",
    "if generated_text_directory not in listdir():\n",
    "    mkdir(generated_text_directory)\n",
    "\n",
    "with open(join(getcwd(), generated_text_directory, generated_text_filename + '.txt'), 'w', encoding='utf-8') as file:\n",
    "    file.write(text[0::2])\n",
    "\n",
    "if 'colab' in str(get_ipython()): files.download(join(getcwd(), generated_text_directory, generated_text_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "generate_new_text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
